{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd, numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import re\n",
    "import janome\n",
    "import jaconv\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "import string\n",
    "import MeCab\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import unicodedata\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from pprint import pprint\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YahooNewsAnalytics(n=30):\n",
    "    i = 0\n",
    "    url = \"https://news.yahoo.co.jp/topics/top-picks\"\n",
    "    URL = \"https://news.yahoo.co.jp/\"\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    all_page_links = []\n",
    "    all_page_links.append(url)\n",
    "    while True:\n",
    "        try:\n",
    "            next = soup.find(\"li\", class_=\"pagination_item-next\").find(\"a\")[\"href\"]\n",
    "            next_link = URL + next\n",
    "            all_page_links.append(next_link)\n",
    "            next_res = requests.get(next_link)\n",
    "            soup = BeautifulSoup(next_res.text, \"html.parser\")\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    all_links = []\n",
    "    for url in all_page_links: # all_page_links: 全てのニュースのリスト\n",
    "        print(url, end='\\n\\n')\n",
    "        res = requests.get(url) # url: 25個分のニュースのリスト\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        page_soup = soup.find_all(\"a\", class_=\"newsFeed_item_link\")\n",
    "        for href in page_soup:\n",
    "            news = []\n",
    "            link = href[\"href\"] # link: 一つのニュースのリンク(本文は一部のみ)\n",
    "            link_res = requests.get(link)\n",
    "            href_soup = BeautifulSoup(link_res.text, \"html.parser\")\n",
    "            title_link = href_soup.find(\"a\", class_=\"sc-eAyhxF\")[\"href\"] # title_link: 本文\n",
    "            news.append(title_link)\n",
    "            res1 = requests.get(title_link)\n",
    "            soup1 = BeautifulSoup(res1.text, \"html.parser\")\n",
    "            while True:\n",
    "                try:\n",
    "                    next_link = soup1.find(\"li\", class_=\"pagination_item-next\").find(\"a\")[\"href\"]\n",
    "                    next_link = URL + next_link\n",
    "                    next_res = requests.get(next_link)\n",
    "                    next_soup = BeautifulSoup(next_res.text, \"html.parser\")\n",
    "                    news.append(next_soup)\n",
    "                except Exception:\n",
    "                    all_links.append(news)\n",
    "                    i += 1\n",
    "                    print('\\r{0}記事'.format(i), end='')\n",
    "                    if i >= n:\n",
    "                        return all_links\n",
    "                    break\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.yahoo.co.jp/topics/top-picks\n",
      "\n",
      "10記事"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['https://headlines.yahoo.co.jp/article?a=20201102-00000004-friday-base'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-00202469-fnn-soci'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-02362953-tospoweb-fight'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-00000007-jij_afp-int'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-21011444-nksports-base'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-00000050-kyodonews-soci'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-00010000-chibatopi-l12'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-00000515-sanspo-ent'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201102-00000004-hbcv-hok'],\n",
       " ['https://headlines.yahoo.co.jp/hl?a=20201101-00000109-mai-soci']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YahooNewsAnalytics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
